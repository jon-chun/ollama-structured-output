# Model configuration
model_config:
  model_temperature: 0.7
  model_top_p: 0.9
  model_max_tokens: 2000

# Execution settings
execution:
  max_calls_per_prompt: 100

# Timeout configuration
timeout:
  max_api_wait_sec: 30
  max_api_timeout_retries: 3
  api_wait_step_increase_sec: 10

# Logging configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(levelname)s - %(message)s"
  file: "evaluation.log"

# Output configuration
output:
  base_dir: "evaluation_results"

# Data configuration
data:
  input_file: "vignettes_renamed_clean.csv"
  train_split: 0.7
  random_seed: 42

# Prompt templates
prompts:
  system1: |
    Please analyze the following risk factors and determine if they indicate high risk:
    
    RISK_FACTORS:
    {risk_factors}
    
    Respond in JSON format with a prediction (YES/NO) and confidence level (0-100).
  
  cot: |
    Please analyze the following risk factors using chain-of-thought reasoning to determine if they indicate high risk:
    
    RISK_FACTORS:
    {risk_factors}
    
    Think through each factor and its implications step by step.
    Respond in JSON format with:
    - prediction (YES/NO)
    - confidence (0-100)
    - risk_factors (list of analyzed factors with weights and reasoning)

# Model ensemble configuration
model_ensemble:
  llama2:
    temperature: 0.7
    max_tokens: 2000
  mixtral:
    temperature: 0.7
    max_tokens: 2000
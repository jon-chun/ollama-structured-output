# config.yaml

# Model configuration
model_parameters:
  model_temperature: 0.7
  model_top_p: 0.9
  model_max_tokens: 2000
  api_timeout: 30

# Execution settings
execution:
  max_calls_per_prompt:  6 # Maximum number of calls per prompt/sample
  batch_size: 3

# Global configuration parameters
flags:
  max_samples: 10  # Maximum number of samples to evaluate (if less than training set size)

# Timeout configuration
timeout:
  max_api_wait_sec: 30
  max_api_timeout_retries: 3
  api_wait_step_increase_sec: 10

# Logging configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(levelname)s - %(message)s"
  file: "evaluation.log"

# Output configuration
output:
  base_dir: "evaluation_results"

# Data configuration
data:
  input_file: "./data/vignettes_renamed_clean.csv"
  train_split: 0.8
  random_seed: 42

# Prompt templates
prompts:
  system1: |
    You are a risk assessment expert. Analyze the following risk factors to determine if they indicate high risk.
    You must respond ONLY with a JSON object in exactly this format, with no additional text:
    
    {
      "prediction": "YES" or "NO",
      "confidence": number between 0 and 100
    }

    Risk Factors to analyze:
    {risk_factors}

  cot: |
    You are a risk assessment expert. Think step by step about these risk factors:
    {risk_factors}

    After your analysis, you must respond ONLY with a JSON object in exactly this format, with no additional text:
    
    {
      "prediction": "YES" or "NO",
      "confidence": number between 0 and 100
    }

# Model ensemble configuration
model_ensemble:
  llama3.2:1b-instruct-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2000
    max_context_window: 8000

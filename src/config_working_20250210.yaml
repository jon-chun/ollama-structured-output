# config.yaml 
model_parameters:
    api_type: 'ollama' # 'groq' # 'anthropic' # 'openai' # 'google' # 'openai'  # Required: one of 'ollama', 'openai', 'anthropic', 'google'
    api_model: 'exaone3.5:2.4b-instruct-q4_K_M' # deepseek-r1-distill-llama-70b' # 'claude-3-5-haiku-20241022' # 'gpt-4o-mini' # 'gemini-2.0-flash' # 'gpt-4o-mini','gemini-2.0-flash' # api_model in ['ollama-all','openai','anthropic','google','together','groq']
    model_temperature: 0.0
    model_top_p: 0.9
    model_max_tokens: 8000
    api_timeout: 300
    # api_key_hardcoded: false  # Optional: already has a default
    # openai_api_key: ''  # Optional: already has a default
    # anthropic_api_key: ''  # Optional: already has a default
    # google_api_key: ''  # Optional: already has a default


# Execution settings
execution:
  generation_type: "binary_classification" # Select from ["code", "open-ended", "binary_class", "multiclass", "regression"]
  max_calls_per_prompt: 1 # e.g. 5 Maximum number of samples for the same prompt
  batch_size: 1 # 2
  nshot_ct: 30 # 30
  save_prompts: True # Unused for now
  require_reasoning: False # Unused for now
  require_reasoning_retry_ct: 3 # Unused for now

# Global configuration parameters: flags and constants
flags:
  max_samples: 100 # 150 how many rows to sample from the input data
  FLAG_PROMPT_PREFIX: False  # Enable prompt prefix
  FLAG_PROMPT_SUFFIX: False  # Enable prompt suffix
  prompt_prefix: "IMPORTANT PREFIX: Please review the following information carefully.\n"
  prompt_suffix: "\nEND OF PROMPT"

# Conditional backstories (scenarios)
# scenario_fairness_demographic_parity: |
#   FAIRNESS ENFORCEMENT - Your recidivism prediction should classify defendants as low-risk at equal rates across all demographic groups. For example, if 30% of defendants from Group A are classified as low-risk, then approximately 30% of defendants from Group B should also be classified as low-risk, regardless of other factors. This ensures different groups have equal probability of receiving a favorable risk assessment. ENSURE your PREDICTION conforms to this definition of FAIRNESS

# scenario_fairness_equalized_odds: |
#   FAIRNESS ENFORCEMENT - Your recidivism prediction should have equal true positive rates and false positive rates across defendant demographics. If you correctly identifies 80% of Group A defendants who actually reoffend and incorrectly flags 10% of Group A defendants who don't reoffend, it should maintain these same rates for Group B defendants. This ensures equal error rates across groups.  ENSURE your PREDICTION conforms to this definition of FAIRNESS

# scenario_fairness_equalized_outcomes: |
#   FAIRNESS ENFORCEMENT - Your recidivism prediction should give equal chances of low-risk predictions to equally qualified defendants across protected groups. If 75% of defendants from Group A with no prior convictions are classified as low-risk, then 75% of defendants from Group B with no prior convictions should also be classified as low-risk. This focuses specifically on equal true positive rates for those who merit favorable risk assessments.  ENSURE your PREDICTION conforms to this definition of FAIRNESS

# Timeout configuration
timeout:
  max_wait_ollama_pull_model_sec: 900  # Add this line
  max_api_wait_sec: 780  # 30 normally, small thiner 120
  max_api_timeout_retries: 3
  api_wait_step_increase_sec: 60
  delay_between_prompt_types_sec: 2
  delay_between_model_load_sec: 2

# Malformed response handling
malformed_reponse:
  save_malformed_responses: True
  retry_malformed_ct: 5 # lower for execution.generation_type=='binary_class' (3-7), higher for =='code' (5-10)

# Logging configuration
logging:
  level: "DEBUG" # "INFO"
  format: "%(asctime)s - %(levelname)s - %(message)s"
  file: "evaluation_long_final_seed64_temp00_20250130.log"

# 2025-01-30 06:39:42,237 - INFO - All evaluations completed in 30336.95 seconds (seed15)

# UPDATE:
#  output['base_dir']: "evaluation_results_long_final_seed64_20250130"
#  data['random_seed']: 64
#  logging['file']: "evaluation.log"

# Output configuration
output:
  # base_dir: "../evaluation_results_long_final_seed42_temp00_20250209"
  # base_dir: "../evaluation_results_long_final_seed81_temp00_20250209"
  base_dir: "../evaluation_results_long_final_seed42_temp00_20250130"
  # base_dir: "evaluation_results_long_20250110" # "evaluation_results_short", "evaluation_results_binary_class", "evaluation_results_multiclass", "evaluation_results_regression"
  # base_dir: "evaluation_results_long_h100_20250115"
  # base_dir: "evaluation_results_short_min_pull_models" #_20250126"
  # base_dir: "evaluation_results_short_20250108" # "evaluation_results_short" 
  # base_dir: "evaluation_results_features-top5_20250112" # "evaluation_results_short"
# Data configuration
data:
  input_file: "../data/vignettes_renamed_clean.csv"
  # input_file: "./data/vignettes_filtered_0_features-6_text_summary.csv"
  risk_factors: "long_text_summary" #"long_text_summary" # "short_text_summary" or "long_text_summary" # change coordinate with output['base_dir'] value
  # risk_factors: "short_text_summary"
  # risk_factors: "text_summary"
  outcome: "target"
  random_seed: 42 # 81, 64, 42, 15, etc.
  train_split: 20  # 20% for training
  validate_split: 20  # 20% for validation
  test_split: 60  # 60% for testing

# Prompt templates
prompts:
  prompt_persona: |
    You are a highly experienced criminal justice risk assessment expert
    employing advanced statistical and judicial knowledge.

  system1: |
    You are a risk assessment expert. Analyze the following risk factors to determine if they indicate high risk.
    You must respond ONLY with a JSON object in exactly this format, with no additional text:
    
    {
      "prediction": "YES" or "NO",
      "confidence": number between 0 and 100
    }

    Risk Factors to analyze:
    {risk_factors}

  cot: |
    You are a criminal risk assessment expert. Step back, take a deep breath and carefully think step by step to assign a relative weight [low, medium, high] to each risk factor before using them to collectively predict 3-year rearrest recidivism:
    {risk_factors}

    After your analysis, you must respond ONLY with a JSON object in exactly this format, with no additional text:
    
    {
      "risk_factors": [
        ("factor1", "low"|"medium"|"high", "textual reasoning"),
        ("factor2", "low"|"medium"|"high", "textual reasoning"),
        ...
      ],
      "prediction": "YES" or "NO",
      "confidence": number between 0 and 100
    }

  cot-nshot: |
    You are a criminal risk assessment expert. First,analyze the following example cases of risk factors and resulting 3 year rearrest recidivism outcome:
    {nshot_example_str}
    
    Next, step back and take a deep breath and carefully think step by step to assign a relative weight [low, medium, high] to each risk factor before using them to collectively predict 3-year rearrest recidivism for this case:
    {risk_factors}

    Based on the above n-shot examples and your weighted risk factor analysis for this particular case, predict the 3-year rearrest recidivism outcome for this case.
    Respond ONLY with a JSON object in exactly this format, with no additional text:
    
    {
      "risk_factors": [
        ("factor1", "low"|"medium"|"high", "textual reasoning"),
        ("factor2", "low"|"medium"|"high", "textual reasoning"),
        ...
      ],
      "prediction": "YES" or "NO",
      "confidence": number between 0 and 100
    }


# Model ensemble configuration 
# Time out:
#   yi:9b-v1.5-q4_K_M: 
# ollama run command-r7b:7b-12-2024-fp16 --verbose
# ollama run command-r7b:7b-12-2024-q4_K_M --verbose
# ollama run olmo2:7b-1124-instruct-q4_K_M --verbose
# ollama run olmo2:7b-1124-instruct-fp16 --verbose
# ollama rolmo2:13b-1124-instruct-q4_K_M --verbose
# ollama run olmo2:13b-1124-instruct-fp16 --verbose
# model_ensemble_completed::  
# ## model_ensemble_completed:

#  model_ensemble_final:
#    "aya-expanse:8b-q4_K_M",
#    "deepseek-r1:7b",
#    "dolphin3:8b-llama3.1-q4_K_M",
#    "exaone3.5:7.8b-instruct-q4_K_M",
#    "falcon3:7b-instruct-q4_K_M",
#    "gemma2:9b-instruct-q4_K_M",
#    "glm4:9b-chat-q4_K_M",
#    "granite3.1-dense:8b-instruct-q4_K_M",
#    "hermes3:8b-llama3.1-q4_K_M",
#    "llama3_1_8b_instruct_q4_k_m",
#    "marco-o1:7b-q4_K_M",
#    "mistral:7b-instruct-q4_K_M",
#    "olmo2:7b-1124-instruct_q4_K_M",
#    "phi4:14b-q4_K_M",
#    "qwen2.5:7b-instruct-q4_K_M",
#    "tulu3:8b-q4_K_M",


model_ensemble:    
  ## athene-v2:72b-q4_K_M:  # Medium
  ##   max_load_time: 600
  ##   max_response_time: 600
  ##   max_num_predict: 2048
  ##   max_context_window: 32000
  # qwen2.5:14b-instruct-q4_K_M:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 3072
  #   max_context_window: 128000
  ## vicuna:33b-q4_K_M:
  ##   max_load_time: 240
  ##   max_response_time: 240
  ##   max_num_predict: 2048
  ##   max_context_window: 8192  
  wizardlm:13b-llama2-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 8192
  # reflection 70b slow: 5 reponses in 5 hrs  
  #  reflection:70b-q4_K_M: 
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 128000
  # qwq:32b-preview-q4_K_M:  # Slow, cot-nshot returns a lot fo unstrctured responses that cannot be parsed
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 32000  
  #  # qwen2.5:72b-instruct-q4_K_M:  # Slow
  #  # qwq:32b-preview-q4_K_M:  # Slow  # Slow
  #  # reflection:70b-q4_K_M:
  # falcon:40b-instruct-q4_0:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8192
  # qwq:32b-preview-q4_K_M:  # Slow, cot-nshot returns a lot fo unstrctured responses that cannot be parsed
  #   max_load_time: 240
  #   max_response_time: 600
  #   max_num_predict: 2048
  #   max_context_window: 32000  
  # # 17.25 tokens/s (15mos)
  # 153.85 tokens/s (15mos)
  # falcon:7b-instruct-q4_0:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8192 
  # 12.09 tokens/s (15mos)
  # alcon:40b-instruct-q4_0:
  #  max_load_time: 240
  #  max_response_time: 240
  #  max_num_predict: 2048
  #  max_context_window: 8192 
  # 138.41 tokens/s (6mos)
  # mistral:7b-instruct-q4_K_M:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8192     
  # 40.76 tokens/s
  ### granite3.1-moe:1b-instruct-q4_K_M:
  ###   max_load_time: 240
  ###   max_response_time: 240
  ###   max_num_predict: 2048
  ###   max_context_window: 8192  
  # tulu3:70b-q4_K_M:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8192

  # 10.92 tokens/s
  # mistral-small:24b-instruct-2501-q4_K_M:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8192
  # 45.96 tokens/s # time out
  # reflection:70b-q4_K_M:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 128000
  # # 6.51 tokens/s
  # hermes3:70b-llama3.1-q4_K_M:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8192
  # 42.35 tokens/s
  # gemma2:2b-instruct-q4_K_M:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8192
  # 203.76 tokens/s
  # granite3.1-dense:2b-instruct-q4_K_M:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 4096
  #   max_context_window: 8000
  # # 6.27 tokens/s
  # nemotron:70b-instruct-q4_K_M:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 4096
  #   max_context_window: 8000
  # deepseek-r1:14b:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8192
  # deepseek-r1:32b:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8192
  # deepseek-r1:70b:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8192 
  #
  # qwq:32b-preview-q4_K_M:  # Slow, cot-nshot returns a lot fo unstrctured responses that cannot be parsed
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 32000  
  #  # # # # model_ensemble:
  # yi:9b-v1.5-q4_K_M:  # Slow, null response
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8192
  # sailor2:1b-chat-q4_K_M:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8000
  # sailor2:8b-chat-q4_K_M:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8000
  # sailor2:20b-chat-q4_K_M:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8000
  # 460.88 tokens/s
  # smollm2:135m-instruct-q4_K_M:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8000
  # 325.84 tokens/s - non responsive system1
  # smollm2:360m-instruct-q4_K_M: 
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8000
  # smollm2:1.7b-instruct-q4_K_M:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 3072
  #   max_context_window: 32768
  # deepseek-r1:8b:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8192
  #
  #  # # # # model_ensemble:
  # yi:9b-v1.5-q4_K_M:  # Slow
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8192
  #
  aya-expanse:8b-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 32000
  # ##### aya-expanse:8b-fp16:  # MUST PULL
  # #####   max_load_time: 240
  # #####   max_response_time: 240
  # #####   max_num_predict: 2048
  # #####   max_context_window: 32000
  aya-expanse:32b-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 32000
  command-r7b:7b-12-2024-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 8192
  # command-r7b:7b-12-2024-fp16:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8192  
  command-r:35b-08-2024-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 8192
  deepseek-r1:1.5b:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 8192
  deepseek-r1:7b:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 8192
  deepseek-r1:8b:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 8192
  deepseek-r1:14b:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 8192
  deepseek-r1:32b:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 8192
  #  deepseek-r1:70b:
  #    max_load_time: 240
  #    max_response_time: 240
  #    max_num_predict: 2048
  #    max_context_window: 8192 
  dolphin3:8b-llama3.1-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 4096
    max_context_window: 32000
  dolphin-mistral:7b-v2-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 8192
  # ### dolphin3:8b-llama3.1-fp16:
  # ###   max_load_time: 240
  # ###   max_response_time: 240
  # ###   max_num_predict: 4096
  # ###   max_context_window: 32000
  # ### dolphin-llama3.1:8b-v2.9.4-Q8_0:
  # ###   max_load_time: 240
  # ###   max_response_time: 240
  # ###   max_num_predict: 4096
  # ###   max_context_window: 32000
  # ### dolphin-mistral-nemo:12b-v2.9.3-F16:
  # ###   max_load_time: 240
  # ###   max_response_time: 240
  # ###   max_num_predict: 4096
  # ###   max_context_window: 16000
  # ### dolphin-2.9.2-qwen2-7b:f16:
  # ###   max_load_time: 240
  # ###   max_response_time: 240
  # ###   max_num_predict: 4096
  # ###   max_context_window: 16000
  exaone3.5:2.4b-instruct-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 32000
  exaone3.5:7.8b-instruct-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 32000
  exaone3.5:32b-instruct-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 128000
  # ##### exaone3.5:7.8b-instruct-fp16:
  # #####   max_load_time: 240
  # #####   max_response_time: 240
  # #####   max_num_predict: 2048
  # #####   max_context_window: 32000
  # #####exaone3.5:2.4b-instruct-fp16:
  # #####  max_load_time: 240
  # #####  max_response_time: 240
  # #####  max_num_predict: 2048
  # #####  max_context_window: 32000
  # #####falcon3:3b-instruct-fp16:
  # #####  max_load_time: 240
  # #####  max_response_time: 240
  # #####  max_num_predict: 2048
  # #####  max_context_window: 8192
  falcon3:7b-instruct-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 8192
  # ##### falcon3:7b-instruct-fp16:
  # #####   max_load_time: 240
  # #####   max_response_time: 240
  # #####   max_num_predict: 2048
  # #####   max_context_window: 8192
  falcon3:10b-instruct-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 8192
  # ##### falcon3:10b-instruct-fp16:
  # #####   max_load_time: 240
  # #####   max_response_time: 240
  # #####   max_num_predict: 2048
  # #####   max_context_window: 8192
  # ##### gemma2:2b-instruct-fp16:
  # #####   max_load_time: 240
  # #####   max_response_time: 240
  # #####   max_num_predict: 2048
  # #####   max_context_window: 8192
  gemma2:9b-instruct-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 8192
  gemma2:9b-instruct-fp16:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 8192
  gemma2:27b-instruct-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 8192
  glm4:9b-chat-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 8192
  granite3.1-dense:2b-instruct-fp16:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 4096
    max_context_window: 8000
  granite3.1-dense:8b-instruct-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 4096
    max_context_window: 8000
  # #####granite3.1-dense:8b-instruct-fp16:
  # #####  max_load_time: 240
  # #####  max_response_time: 240
  # #####  max_num_predict: 4096
  # #####  max_context_window: 8000
  granite3.1-moe:3b-instruct-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 8192
  hermes3:3b-llama3.2-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 8192
  # 122.19 tokens/s
  hermes3:8b-llama3.1-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 8192
  # hermes3:8b-llama3.1-fp16:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8192
  # 72.87 tokens/s (13mos)
  llama2:13b-chat-q5_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 8192  
  llama3.2:1b-instruct-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 4096
    max_context_window: 128000
  # llama3.2:1b-instruct-fp16:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 4096
  #   max_context_window: 128000
  llama3.2:3b-instruct-q4_K_M:
    max_loaollama run command-r7b:7b-12-2024-q4_K_M --verbosed_time: 240
    max_response_time: 240
    max_num_predict: 4096
    max_context_window: 128000
  # llama3.2:3b-instruct-fp16:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 4096
  #   max_context_window: 128000
  llama3.1:8b-instruct-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 4096
    max_context_window: 32000
  # llama3.1:8b-instruct-fp16:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 4096
  #   max_context_window: 32000
  llama3.1:70b-instruct-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 4096
    max_context_window: 32000
  llama3.3:70b-instruct-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 4096
    max_context_window: 128000
  marco-o1:7b-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 4096
  # marco-o1:7b-fp16:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 4096
  mistral:7b-instruct-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 4096
    max_context_window: 16000
  # mistral:7b-instruct-fp16:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 4096
  #   max_context_window: 16000
  mistral-nemo:12b-instruct-2407-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 4096
    max_context_window: 128000
  mistral-small:22b-instruct-2409-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 4096
    max_context_window: 128000
  mixtral:8x7b-instruct-v0.1-q4_K_M:  # MUST PULL
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 4096
    max_context_window: 16000
  nemotron-mini:4b-instruct-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 4096
    max_context_window: 16000
  # nemotron-mini:4b-instruct-fp16:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 4096
  #   max_context_window: 16000
  olmo2:7b-1124-instruct-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 8192
  # ##### olmo2:7b-1124-instruct-fp16:
  # #####   max_load_time: 240
  # #####   max_response_time: 240
  # #####   max_num_predict: 2048
  # #####   max_context_window: 8192
  olmo2:13b-1124-instruct-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 8192
  # ##### olmo2:13b-1124-instruct-fp16:
  # #####   max_load_time: 240
  # #####   max_response_time: 240
  # #####   max_num_predict: 2048
  # #####   max_context_window: 8192
  phi4:14b-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 3072
    max_context_window: 128000
  # ##### phi4:14b-fp16:
  # #####   max_load_time: 240
  # #####   max_response_time: 240
  # #####   max_num_predict: 3072
  # #####   max_context_window: 128000
  phi3.5:3.8b-mini-instruct-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 8192
  # ##### phi3.5:3.8b-mini-instruct-fp16:
  # #####   max_load_time: 240
  # #####   max_response_time: 240
  # #####   max_num_predict: 2048
  # #####   max_context_window: 8192
  # qwen2.5:0.5b time out 
  qwen2.5:0.5b-instruct-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 3072
    max_context_window: 32000
  qwen2.5:1.5b-instruct-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 3072
    max_context_window: 32000
  qwen2.5:3b-instruct-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 3072
    max_context_window: 32000
  qwen2.5:7b-instruct-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 3072
    max_context_window: 128000
  # ##### qwen2.5:7b-instruct-fp16:
  # #####   max_load_time: 240
  # #####   max_response_time: 240
  # #####   max_num_predict: 3072
  # #####   max_context_window: 128000
  qwen2.5:14b-instruct-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 3072
    max_context_window: 128000
  qwen2.5:32b-instruct-q4_K_M:  # Slow
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 3072
    max_context_window: 128000
  # qwen2.5:72b-instruct-q4_K_M:  # Slow
  #   max_load_time: 240
  #   max_response_time: 240 
  #   max_num_predict: 3072
  #   max_context_window: 128000
  #  # qwq:32b-preview-q4_K_M:  # Slow  # Slow
  #  # reflection:70b-q4_K_M:
  # qwq:32b-preview-q4_K_M:  # Slow, cot-nshot returns a lot fo unstrctured responses that cannot be parsed
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 32000  
  # #  # # # # model_ensemble:
  # yi:9b-v1.5-q4_K_M:  # Slow
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8192
  sailor2:1b-chat-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 8000
  sailor2:8b-chat-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 8000
  sailor2:20b-chat-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 8000
  # #  # smallthinker:3b-preview-q4_K_M:  # Medium
  # #  # smallthinker:3b-preview-fp16:  # Medium
  smallthinker:3b-preview-q4_K_M:  # Medium, incr timeout:max_api_wait_sec from default 30 to 120 sec
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 3072
    max_context_window: 32768
  # # 460.88 tokens/s
  # smollm2:135m-instruct-q4_K_M:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8000
  # # 325.84 tokens/s
  # smollm2:360m-instruct-q4_K_M:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8000
  # smollm2:1.7b-instruct-q4_K_M:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 3072
  #   max_context_window: 32768
  # ##### smollm2:1.7b-instruct-fp16:
  # #####   max_load_time: 240
  # #####   max_response_time: 240
  # #####   max_num_predict: 3072
  # #####   max_context_window: 32768
  solar-pro:22b-preview-instruct-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 3072
    max_context_window: 32768
  tulu3:8b-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 8192
  # ##### tulu3:8b-fp16:
  # #####   max_load_time: 240
  # #####   max_response_time: 240
  # #####   max_num_predict: 2048
  # #####   max_context_window: 8192
  # model_ensemble:
  tulu3:70b-q4_K_M:
    max_load_time: 240
    max_response_time: 240
    max_num_predict: 2048
    max_context_window: 8192
  # ###### athene-v2:72b-q4_K_M:  # Medium
  # ######   max_load_time: 240
  # ######   max_response_time: 240
  # ######   max_num_predict: 2048
  # ######   max_context_window: 32000
  # deepseek-r1:7b:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8192
  # deepseek-r1:8b:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8192
  # deepseek-r1:14b:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8192
  # deepseek-r1:32b:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8192
  # deepseek-r1:70b:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8192 
  #
  # qwen2.5:72b-instruct-q4_K_M:  # Slow
  #   max_load_time: 240
  #   max_response_time: 240 
  #   max_num_predict: 3072
  #   max_context_window: 128000
  # reflection:70b-q4_K_M:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 128000
  # smallthinker:3b-preview-q4_K_M:  # Medium, incr timeout:max_api_wait_sec from default 30 to 120 sec
  #  max_load_time: 240
  #  max_response_time: 240
  #  max_num_predict: 3072
  #  max_context_window: 32768
  # ##### smallthinker:3b-preview-fp16:  # Medium
  # #####   max_load_time: 240
  # #####   max_response_time: 240
  # #####   max_num_predict: 3072
  # #####   max_context_window: 32768
  #
  # yi:34b-chat-q4_K_M:
  #   max_load_time: 240
  #   max_response_time: 240
  #   max_num_predict: 2048
  #   max_context_window: 8192  
  #  # ## phi3.5:3.8b-mini-instruct-q4_K_M:  # Malformed
  #  # ##   max_load_time: 240
  #  # ##   max_response_time: 240
  #  # ##   max_num_predict: 3072
  #  # ##   max_context_window: 128000
  #  # ## phi3.5:3.8b-mini-instruct-fp16:  # Malformed
  #  # ##   max_load_time: 240
  #  # ##   max_response_time: 240
  #  # ##   max_num_predict: 3072
  #  # ##   max_context_window: 128000
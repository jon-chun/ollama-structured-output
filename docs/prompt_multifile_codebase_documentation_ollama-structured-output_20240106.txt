###CODE: setup.py
from setuptools import setup, find_packages

setup(
    name="llm_decisions",
    version="0.1.0",
    packages=find_packages(),
    install_requires=[
        'pydantic>=2.0.0',
        'PyYAML>=5.1',
        'aiofiles>=0.6.0',
        'ollama>=0.1.0',
    ],
    python_requires='>=3.8',
    author="Your Name",
    author_email="your.email@example.com",
    description="A package for evaluating language model decisions",
    long_description=open('README.md').read(),
    long_description_content_type="text/markdown",
    classifiers=[
        "Development Status :: 3 - Alpha",
        "Intended Audience :: Developers",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
    ],
)

###CODE: models.py
# models.py
from enum import Enum
from typing import List
from pydantic import BaseModel, Field

class PromptType(str, Enum):
    """Enumeration of supported prompt types"""
    SYSTEM1 = 'system1'
    COT = 'cot'
    COT_NSHOT = 'cot-nshot'  # New prompt type for n-shot learning

class RiskWeight(str, Enum):
    """Enumeration for risk weight levels"""
    HIGH = 'high'
    MEDIUM = 'medium'
    LOW = 'low'

class RiskFactor(BaseModel):
    """Model for individual risk factors and their assessment"""
    factor: str = Field(..., min_length=1)
    weight: RiskWeight
    reasoning: str = Field(..., min_length=5)

class Decision(BaseModel):
    """Base decision model with prediction and confidence"""
    prediction: str = Field(
        ...,
        description="The model's prediction (YES/NO)",
        pattern="^(YES|NO)$"
    )
    confidence: int = Field(
        ...,
        description="Confidence level (0-100)",
        ge=0,
        le=100
    )

class DecisionWithRiskFactors(Decision):
    """Extended decision model with risk factor analysis"""
    risk_factors: List[RiskFactor] = Field(
        ...,
        description="Analyzed risk factors with weights and reasoning"
    )

###CODE: prompt_manager.py
import logging
from typing import Dict, List
import random

from config import Config
from data_manager import DataManager
from models import PromptType

class PromptManager:
    """Manages dynamic prompt generation and templating"""
    def __init__(self, config: Config, data_manager: DataManager):
        self.config = config
        self.data_manager = data_manager
        self._base_prompts = {
            PromptType.SYSTEM1: config.prompts["system1"],
            PromptType.COT: config.prompts["cot"],
            PromptType.COT_NSHOT: config.prompts["cot-nshot"]
        }
        self._nshot_examples = None  # Cache for n-shot examples
        
    def _generate_nshot_examples(self) -> str:
        """Generate n-shot example string from validation set"""
        if self._nshot_examples is not None:
            return self._nshot_examples
            
        # Get validation data
        validate_data = self.data_manager.get_batch(None, dataset='validate')
        
        # Determine number of examples to use
        n = min(self.config.nshot_ct, len(validate_data))
        
        # Randomly select examples
        selected_examples = random.sample(validate_data, n)
        
        # Build example string
        examples_str = ""
        for i, example in enumerate(selected_examples, 1):
            examples_str += f"""
Example #{i}:
Risk Factors: {example['risk_factors']}
Outcome: {example['target']}
"""
        
        # Cache the examples
        self._nshot_examples = examples_str
        return examples_str
    
    def get_prompt(self, prompt_type: PromptType, row_id: int) -> str:
        """
        Get a prompt with dynamically inserted risk factors from data.
        Now supports n-shot examples for COT_NSHOT prompt type.
        """
        if prompt_type not in self._base_prompts:
            raise ValueError(f"Invalid prompt type: {prompt_type}")
            
        base_prompt = self._base_prompts[prompt_type]
        
        try:
            # Get risk factors text from training data
            risk_factors = self.data_manager.get_risk_factors(row_id)
            
            # Handle n-shot examples if needed
            if prompt_type == PromptType.COT_NSHOT:
                nshot_examples = self._generate_nshot_examples()
                prompt = base_prompt.replace('{nshot_example_str}', nshot_examples)
            else:
                prompt = base_prompt
                
            # Replace risk factors placeholder
            prompt = prompt.replace('{risk_factors}', risk_factors)
            prompt = prompt.replace('###RISK_FACTORS:', risk_factors)
            
            logging.debug(f"Generated {prompt_type} prompt for row {row_id}")
            return prompt
            
        except Exception as e:
            logging.error(f"Error generating prompt for row {row_id}: {str(e)}")
            raise

###CODE: manager.py
# data_manager.py
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import logging
from typing import Tuple

class DataManager:
    """Manages data loading, splitting, and preprocessing for model evaluation"""
    def __init__(self, config: 'Config'):
        self.config = config
        self.data_path = Path(config.data["input_file"])
        self.df = None
        self.df_train = None
        self.df_test = None
        
    def load_and_prepare_data(self) -> None:
        """Load CSV data and prepare train/test splits"""
        # Read CSV and add sequence ID
        self.df = pd.read_csv(self.data_path)
        self.df['id'] = np.arange(len(self.df))
        
        # Set random seed and shuffle
        np.random.seed(self.config.data["random_seed"])
        self.df = self.df.sample(frac=1).reset_index(drop=True)
        
        # Split into train/test sets
        train_size = self.config.data["train_split"]
        self.df_train, self.df_test = train_test_split(
            self.df, 
            train_size=train_size,
            random_state=self.config.data["random_seed"]
        )
        
        logging.info(f"Loaded {len(self.df)} total samples")
        logging.info(f"Split into {len(self.df_train)} train and {len(self.df_test)} test samples")


###CODE: metrics.py
# metrics.py
from dataclasses import dataclass
from typing import Dict, Any, Optional

@dataclass
class TimeoutMetrics:
    """Records metrics related to timeout handling and retries"""
    occurred: bool = False
    retry_count: int = 0
    total_timeout_duration: float = 0.0
    final_timeout_duration: Optional[float] = None

@dataclass
class PromptMetrics:
    """Records metrics for a single prompt execution attempt"""
    attempt_number: int
    execution_time_seconds: float
    successful: bool
    timeout_metrics: TimeoutMetrics
    error_message: Optional[str] = None
    prediction: Optional[str] = None
    confidence: Optional[float] = None
    meta_data: Optional[Dict[str, Any]] = None

    def __post_init__(self):
        """Ensure proper type conversion for metrics"""
        self.execution_time_seconds = float(self.execution_time_seconds)
        if self.prediction is not None:
            self.prediction = str(self.prediction)
        if self.confidence is not None:
            self.confidence = float(self.confidence)

@dataclass
class TimeoutStats:
    """Statistics related to timeout occurrences"""
    total_timeouts: int
    avg_timeout_duration: float
    max_timeout_duration: float
    total_timeout_duration: float

###CODE: performance.py
# performance.py
from dataclasses import dataclass, asdict
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from statistics import mean, median, stdev
import logging
import json
import pandas as pd
from pathlib import Path
from sklearn.metrics import confusion_matrix, roc_auc_score

from metrics import TimeoutMetrics, PromptMetrics, TimeoutStats


@dataclass
class PerformanceStats:
    """Comprehensive statistics for an evaluation session."""
    prompt_type: str
    model_name: str
    start_time: datetime
    end_time: datetime
    total_attempts: int
    successful_attempts: int
    failed_attempts: int
    timeout_attempts: int
    avg_execution_time: float
    median_execution_time: float
    sd_execution_time: float
    timeout_stats: TimeoutStats
    meta_data_averages: Dict[str, float]
    meta_data_sd: Dict[str, float]
    prediction_accuracy: float = 0.0
    prediction_distribution: Dict[str, int] = None
    actual_distribution: Dict[str, int] = None
    confusion_matrix: Dict[str, int] = None  # tp, tn, fp, fn
    auc_roc: float = 0.0


class DecisionTracker:
    """Tracks and accumulates prediction statistics across samples."""
    def __init__(self):
        self.total_predictions = 0
        self.correct_predictions = 0
        self.actual_values = []
        self.predicted_values = []
        self.confidences = []

    def record_prediction(self, prediction: str, actual: str, confidence: Optional[float] = None):
        """Record a single prediction & actual."""
        self.total_predictions += 1
        if prediction.upper() == actual.upper():
            self.correct_predictions += 1
        self.actual_values.append(actual)
        self.predicted_values.append(prediction)
        self.confidences.append(confidence if confidence is not None else 0.5)

    def get_accuracy(self) -> float:
        if self.total_predictions == 0:
            return 0.0
        return 100.0 * self.correct_predictions / self.total_predictions

    def get_stats(self) -> Dict[str, Any]:
        """Return comprehensive prediction statistics."""
        return {
            'total_predictions': self.total_predictions,
            'correct_predictions': self.correct_predictions,
            'accuracy': self.get_accuracy(),
            'actual_distribution': pd.Series(self.actual_values).value_counts().to_dict(),
            'predicted_distribution': pd.Series(self.predicted_values).value_counts().to_dict(),
            'actual_values': self.actual_values,
            'predicted_values': self.predicted_values,
            'confidences': self.confidences
        }


class PerformanceTracker:
    """Maintains metrics about model performance over many attempts."""
    def __init__(self, prompt_type: str, model_name: str):
        self.prompt_type = prompt_type
        self.model_name = model_name
        self.metrics: List[PromptMetrics] = []
        self.attempts: List[PromptMetrics] = []  # For backwards compatibility
        self.start_time = datetime.now()
        self.decision_tracker = DecisionTracker()

    def record_attempt(self, metrics: PromptMetrics):
        """Record results from a single attempt."""
        self.metrics.append(metrics)
        self.attempts.append(metrics)  # For backwards compatibility
        status = "successful" if metrics.successful else "failed"

        timeout_info = ""
        if metrics.timeout_metrics.occurred:
            timeout_info = (
                f"(timeout occurred, {metrics.timeout_metrics.retry_count} retries, "
                f"total: {metrics.timeout_metrics.total_timeout_duration:.2f}s)"
            )

        logging.debug(
            f"Attempt #{metrics.attempt_number} {status} - "
            f"execution time: {metrics.execution_time_seconds:.2f}s {timeout_info}"
        )

        # Record prediction if available
        if metrics.successful and metrics.prediction is not None:
            actual_value = metrics.meta_data.get('actual_value') if metrics.meta_data else None
            if actual_value:
                self.decision_tracker.record_prediction(
                    metrics.prediction,
                    actual_value,
                    metrics.confidence
                )

    def _calculate_timeout_stats(self) -> TimeoutStats:
        """Compute aggregated stats about timeouts."""
        timeout_durations = [
            m.timeout_metrics.total_timeout_duration
            for m in self.metrics if m.timeout_metrics.occurred
        ]
        if not timeout_durations:
            return TimeoutStats(0, 0.0, 0.0, 0.0)

        return TimeoutStats(
            total_timeouts=len(timeout_durations),
            avg_timeout_duration=mean(timeout_durations),
            max_timeout_duration=max(timeout_durations),
            total_timeout_duration=sum(timeout_durations)
        )

    def _calculate_meta_data_stats(self) -> Tuple[Dict[str, float], Dict[str, float]]:
        """Calculate averages and std-dev for numeric metadata."""
        meta_data_values = {}
        for m in self.metrics:
            if m.meta_data is not None:
                for key, val in m.meta_data.items():
                    if isinstance(val, (int, float)):
                        if key not in meta_data_values:
                            meta_data_values[key] = []
                        meta_data_values[key].append(float(val))

        averages = {}
        std_devs = {}
        for key, values in meta_data_values.items():
            if values:
                avg_ = mean(values)
                sd_ = stdev(values) if len(values) > 1 else 0.0
                averages[key] = avg_
                std_devs[key] = sd_

        return averages, std_devs

    def _generate_stats(self) -> Optional[PerformanceStats]:
        """Compute final PerformanceStats from all recorded attempts."""
        if not self.metrics:
            return None

        execution_times = [m.execution_time_seconds for m in self.metrics]
        meta_averages, meta_sds = self._calculate_meta_data_stats()
        decision_stats = self.decision_tracker.get_stats()

        # Build confusion matrix data
        y_true = [1 if v.upper() == "YES" else 0 for v in decision_stats['actual_values']]
        y_pred = [1 if v.upper() == "YES" else 0 for v in decision_stats['predicted_values']]

        # Calculate confusion matrix
        if y_true and y_pred:
            tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
            confusion_dict = {
                "tp": int(tp),
                "tn": int(tn),
                "fp": int(fp),
                "fn": int(fn)
            }
        else:
            confusion_dict = {"tp": 0, "tn": 0, "fp": 0, "fn": 0}

        # Calculate AUC-ROC
        auc_roc = 0.0
        if y_true and len(set(y_true)) > 1:
            auc_roc = roc_auc_score(y_true, decision_stats['confidences'])

        return PerformanceStats(
            prompt_type=str(self.prompt_type),
            model_name=self.model_name,
            start_time=self.start_time,
            end_time=datetime.now(),
            total_attempts=len(self.metrics),
            successful_attempts=sum(1 for m in self.metrics if m.successful),
            failed_attempts=sum(1 for m in self.metrics if not m.successful),
            timeout_attempts=sum(1 for m in self.metrics if m.timeout_metrics.occurred),
            avg_execution_time=mean(execution_times) if execution_times else 0.0,
            median_execution_time=median(execution_times) if execution_times else 0.0,
            sd_execution_time=stdev(execution_times) if len(execution_times) > 1 else 0.0,
            timeout_stats=self._calculate_timeout_stats(),
            meta_data_averages=meta_averages,
            meta_data_sd=meta_sds,
            prediction_accuracy=decision_stats['accuracy'],
            prediction_distribution=decision_stats['predicted_distribution'],
            actual_distribution=decision_stats['actual_distribution'],
            confusion_matrix=confusion_dict,
            auc_roc=auc_roc
        )

    def save_metrics(self, execution_time: float):
        """Save the final stats for this session as JSON and plain text."""
        stats = self._generate_stats()
        if stats is None:
            logging.warning("No metrics to save - no attempts recorded")
            return

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_base = Path("metrics")
        output_base.mkdir(parents=True, exist_ok=True)

        # Save JSON format
        json_path = output_base / f"metrics_{self.model_name}_{self.prompt_type}_{timestamp}.json"
        with open(json_path, 'w') as f:
            json.dump(asdict(stats), f, indent=2, default=str)

        # Save text report
        self._save_text_report(stats, execution_time, timestamp, output_base)

    def _save_text_report(
        self,
        stats: PerformanceStats,
        execution_time: float,
        timestamp: str,
        output_dir: Path
    ):
        """Generate a readable text report of the performance stats."""
        report_path = output_dir / f"report_{stats.model_name}_{stats.prompt_type}_{timestamp}.txt"
        with open(report_path, 'w') as f:
            f.write(f"Performance Report - {stats.model_name}\n")
            f.write("=" * 50 + "\n\n")

            f.write(f"Prompt Type: {stats.prompt_type}\n")
            f.write(f"Total Duration: {execution_time:.2f}s\n")
            f.write(f"Start Time: {stats.start_time}\n")
            f.write(f"End Time: {stats.end_time}\n\n")

            f.write("Execution Statistics\n")
            f.write("-" * 20 + "\n")
            f.write(f"Total Attempts: {stats.total_attempts}\n")
            f.write(f"Successful: {stats.successful_attempts}\n")
            f.write(f"Failed: {stats.failed_attempts}\n")
            f.write(f"Timeouts: {stats.timeout_attempts}\n\n")

            f.write("Prediction Performance\n")
            f.write("-" * 20 + "\n")
            f.write(f"Accuracy: {stats.prediction_accuracy:.2f}%\n\n")

            f.write("Prediction Distribution:\n")
            for value, count in (stats.prediction_distribution or {}).items():
                f.write(f"  {value}: {count}\n")

            f.write("\nActual Distribution:\n")
            for value, count in (stats.actual_distribution or {}).items():
                f.write(f"  {value}: {count}\n")

            f.write("\nClassification Metrics\n")
            f.write("-" * 20 + "\n")
            cm = stats.confusion_matrix
            f.write(f"True Positives (TP): {cm['tp']}\n")
            f.write(f"False Positives (FP): {cm['fp']}\n")
            f.write(f"True Negatives (TN): {cm['tn']}\n")
            f.write(f"False Negatives (FN): {cm['fn']}\n")
            f.write(f"AUC-ROC: {stats.auc_roc:.4f}\n\n")

            f.write("Timing Statistics\n")
            f.write("-" * 20 + "\n")
            f.write(f"Average Execution: {stats.avg_execution_time:.4f}s\n")
            f.write(f"Median Execution: {stats.median_execution_time:.4f}s\n")
            f.write(f"Std Dev Execution: {stats.sd_execution_time:.4f}s\n")

            if stats.timeout_stats.total_timeouts > 0:
                f.write("\nTimeout Statistics\n")
                f.write("-" * 20 + "\n")
                f.write(f"Total Timeouts: {stats.timeout_stats.total_timeouts}\n")
                f.write(f"Average Duration: {stats.timeout_stats.avg_timeout_duration:.4f}s\n")
                f.write(f"Maximum Duration: {stats.timeout_stats.max_timeout_duration:.4f}s\n")
                f.write(f"Total Duration: {stats.timeout_stats.total_timeout_duration:.4f}s\n")

            if stats.meta_data_averages:
                f.write("\nModel Metadata Statistics\n")
                f.write("-" * 20 + "\n")
                f.write(f"{'Metric':<25} {'Average':<12} {'Std Dev':<12}\n")
                f.write("-" * 51 + "\n")
                for key in stats.meta_data_averages:
                    avg = stats.meta_data_averages[key]
                    sd = stats.meta_data_sd[key]
                    f.write(f"{key:<25} {avg:>11.4f} {sd:>11.4f}\n")


def save_aggregate_stats(session_results: List[PerformanceStats], total_duration: float):
    """Summarize multiple sessions into an aggregate report."""
    if not session_results:
        logging.warning("No session results to aggregate")
        return

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_base = Path("metrics")
    output_base.mkdir(parents=True, exist_ok=True)

    total_predictions = sum(s.successful_attempts for s in session_results)
    total_timeouts = sum(s.timeout_stats.total_timeouts for s in session_results)
    avg_accuracy = mean(s.prediction_accuracy for s in session_results)

    # Aggregate by model
    model_stats = {}
    for stat in session_results:
        if stat.model_name not in model_stats:
            model_stats[stat.model_name] = {
                'total_attempts': 0,
                'successful_attempts': 0,
                'accuracy': [],
                'avg_execution_time': [],
                'auc_roc': [],
                'timeouts': 0
            }
        ms = model_stats[stat.model_name]
        ms['total_attempts'] += stat.total_attempts
        ms['successful_attempts'] += stat.successful_attempts
        ms['accuracy'].append(stat.prediction_accuracy)
        ms['avg_execution_time'].append(stat.avg_execution_time)
        ms['auc_roc'].append(stat.auc_roc)
        ms['timeouts'] += stat.timeout_stats.total_timeouts

    # Calculate model-level summaries
    for model, stats in model_stats.items():
        stats['avg_accuracy'] = mean(stats['accuracy']) if stats['accuracy'] else 0.0
        stats['avg_execution_time'] = mean(stats['avg_execution_time']) if stats['avg_execution_time'] else 0.0
        stats['avg_auc_roc'] = mean(stats['auc_roc']) if stats['auc_roc'] else 0.0
        if stats['total_attempts'] > 0:
            stats['success_rate'] = (stats['successful_attempts'] / stats['total_attempts']) * 100
        else:
            stats['success_rate'] = 0.0

    # Save JSON output
    aggregate_data = {
        'timestamp': timestamp,
        'total_duration': total_duration,
        'total_sessions': len(session_results),
        'total_predictions': total_predictions,
        'total_timeouts': total_timeouts,
        'average_accuracy': avg_accuracy,
        'model_performance': model_stats
    }

    json_path = output_base / f"aggregate_stats_{timestamp}.json"
    with open(json_path, 'w') as f:
        json.dump(aggregate_data, f, indent=2, default=str)

    # Generate text report
    report_path = output_base / f"aggregate_report_{timestamp}.txt"
    with open(report_path, 'w') as f:
        f.write("Aggregate Performance Report\n")
        f.write("=" * 50 + "\n\n")

        f.write("Overall Statistics\n")
        f.write("-" * 20 + "\n")
        f.write(f"Total Duration: {total_duration:.2f}s\n")
        f.write(f"Total Sessions: {len(session_results)}\n")
        f.write(f"Total Predictions: {total_predictions}\n")
        f.write(f"Total Timeouts: {total_timeouts}\n")
        f.write(f"Average Accuracy: {avg_accuracy:.2f}%\n\n")

        f.write("Model Performance\n")
        f.write("-" * 20 + "\n")
        for model_name, stats in model_stats.items():
            f.write(f"\n{model_name}:\n")
            f.write(f"  Total Attempts: {stats['total_attempts']}\n")
            f.write(f"  Successful Attempts: {stats['successful_attempts']}\n")
            f.write(f"  Success Rate: {stats['success_rate']:.2f}%\n")
            f.write(f"  Average Accuracy: {stats['avg_accuracy']:.2f}%\n")
            f.write(f"  Average AUC-ROC: {stats['avg_auc_roc']:.4f}\n")
            f.write(f"  Average Execution Time: {stats['avg_execution_time']:.4f}s\n")
            f.write(f"  Total Timeouts: {stats['timeouts']}\n")

        # Breakdown by prompt type
        prompt_stats = {}
        for stat in session_results:
            if stat.prompt_type not in prompt_stats:
                prompt_stats[stat.prompt_type] = {
                    'accuracies': [],
                    'execution_times': [],
                    'timeout_counts': [],
                    'auc_rocs': []
                }
            ps = prompt_stats[stat.prompt_type]
            ps['accuracies'].append(stat.prediction_accuracy)
            ps['execution_times'].append(stat.avg_execution_time)
            ps['timeout_counts'].append(stat.timeout_stats.total_timeouts)
            ps['auc_rocs'].append(stat.auc_roc)

        f.write("\nPrompt Type Performance\n")
        f.write("-" * 20 + "\n")
        for prompt_type, stats in prompt_stats.items():
            f.write(f"\n{prompt_type}:\n")
            if stats['accuracies']:
                f.write(f"  Average Accuracy: {mean(stats['accuracies']):.2f}%\n")
            if stats['execution_times']:
                f.write(f"  Average Execution Time: {mean(stats['execution_times']):.4f}s\n")
            if stats['auc_rocs']:
                f.write(f"  Average AUC-ROC: {mean(stats['auc_rocs']):.4f}\n")
            f.write(f"  Total Timeouts: {sum(stats['timeout_counts'])}\n")

###CODE: decision.py
# decision.py

import json
import logging
from typing import Optional, Tuple, Dict, Any
import asyncio
from datetime import datetime
from pathlib import Path

from pydantic import BaseModel, Field, ValidationError
from ollama import chat

from config import Config
from models import Decision, PromptType
from metrics import TimeoutMetrics
from utils import clean_model_name, pydantic_or_dict, convert_ns_to_s, check_existing_decision

class MetaData(BaseModel):
    """Model to represent metadata from an API response."""
    model: Optional[str] = Field(None, description="Model name")
    created_at: Optional[str] = Field(None, description="Creation timestamp")
    done_reason: Optional[str] = Field(None, description="Completion reason")
    done: Optional[bool] = Field(None, description="Completion status")
    total_duration: Optional[float] = Field(None, ge=0, description="Total duration in seconds")
    load_duration: Optional[float] = Field(None, ge=0, description="Model loading duration")
    prompt_eval_count: Optional[int] = Field(None, ge=0, description="Prompt token count")
    prompt_eval_duration: Optional[float] = Field(None, ge=0, description="Prompt evaluation duration")
    eval_count: Optional[int] = Field(None, ge=0, description="Response token count")
    eval_duration: Optional[float] = Field(None, ge=0, description="Response generation duration")

def generate_output_path(
    model_name: str,
    prompt_type: PromptType,
    row_id: int,
    repeat_index: int,
    timestamp: str,
    config: Config,
    output_dir: Path,
    nshot_ct: Optional[int] = None
) -> Path:
    """Generate the output path for a decision file."""
    if prompt_type == PromptType.COT_NSHOT:
        filename = (
            f"{model_name}_{prompt_type}_id{row_id}_"
            f"nshot{nshot_ct}_{timestamp}.json"
        )
    else:
        filename = (
            f"{model_name}_{prompt_type}_id{row_id}_"
            f"ver{repeat_index}_{timestamp}.json"
        )
    return output_dir / filename

def process_model_response(response_text: str, model_name: str) -> Dict:
    """
    Attempt multiple parsing strategies to interpret the model output as JSON
    with {'prediction': 'YES'|'NO', 'confidence': 0..100, 'risk_factors': [...] }.
    """
    try:
        logging.debug(f"Raw model response: {response_text}")
        clean_text = response_text.replace('```json', '').replace('```', '')
        parsed_response = None

        # Strategy 1: Direct JSON parsing
        try:
            parsed_response = json.loads(clean_text.strip())
            logging.debug("Successfully parsed response as JSON")
        except json.JSONDecodeError:
            logging.debug("Failed to parse as direct JSON")

        # Strategy 2: Extract JSON from text using regex
        if not parsed_response:
            import re
            json_pattern = r'\{.*\}'
            matches = re.findall(json_pattern, clean_text, re.DOTALL)
            for match in matches:
                try:
                    parsed_response = json.loads(match)
                    logging.debug("Extracted and parsed JSON content using regex")
                    break
                except json.JSONDecodeError:
                    continue

        # Strategy 3: Structured text parsing for specific fields
        if not parsed_response:
            import re
            prediction_match = re.search(r'prediction[\s:"]*([YN]ES|NO)', clean_text, re.IGNORECASE)
            confidence_match = re.search(r'confidence[\s:"]*(\d+)', clean_text)
            risk_factors_match = re.search(r'risk_factors[\s:"]*(\[.*\])', clean_text, re.IGNORECASE)
            if prediction_match and confidence_match:
                parsed_response = {
                    "prediction": prediction_match.group(1).upper(),
                    "confidence": int(confidence_match.group(1))
                }
                if risk_factors_match:
                    parsed_response["risk_factors"] = json.loads(risk_factors_match.group(1))
                logging.debug("Parsed structured text response")
        
        if not parsed_response:
            raise ValueError("Could not extract valid response from model output")

        # Normalize the response
        normalized = {}
        pred_value = str(parsed_response.get('prediction', '')).upper()
        normalized['prediction'] = (
            pred_value if pred_value in ['YES', 'NO']
            else 'YES' if any(ind in pred_value for ind in ['YES', 'TRUE', '1', 'HIGH'])
            else 'NO'
        )

        # Handle confidence
        conf_value = parsed_response.get('confidence', None)
        if conf_value is not None:
            if isinstance(conf_value, str):
                conf_value = float(conf_value.replace('%', ''))
            normalized['confidence'] = int(min(max(float(conf_value), 0.0), 100.0))
        else:
            normalized['confidence'] = 90  # Default confidence

        # Handle risk_factors
        normalized['risk_factors'] = parsed_response.get('risk_factors', None)

        logging.debug(f"Normalized response: {normalized}")
        return normalized

    except Exception as e:
        logging.error(f"Error processing model: {str(model_name)}")
        logging.error(f"Error processing model response: {str(e)}")
        logging.error(f"Raw response: {response_text}")
        raise

async def get_decision(
    prompt_type: PromptType,
    model_name: str,
    config: Config,
    prompt: str
) -> Tuple[Optional[Decision], Optional[MetaData], str, Dict[str, Any]]:
    """Returns a tuple of (Decision, MetaData, used_prompt, extra_data)."""
    try:
        system_message = config.prompts.get("prompt_persona", "") or (
            "You are a risk assessment expert. Your responses must be in valid JSON format "
            "containing exactly three fields: 'risk_factors', 'prediction', and 'confidence'."
        )

        response = await asyncio.to_thread(
            chat,
            messages=[
                {'role': 'system', 'content': system_message},
                {'role': 'user', 'content': prompt}
            ],
            model=model_name,
            options={
                'temperature': config.model_parameters["model_temperature"],
                'top_p': config.model_parameters["model_top_p"],
                'max_tokens': config.model_parameters["model_max_tokens"],
            }
        )

        if not hasattr(response, 'message') or not hasattr(response.message, 'content'):
            raise ValueError("Invalid API response structure")

        normalized_response = process_model_response(response.message.content, model_name)

        try:
            decision = Decision(
                prediction=normalized_response['prediction'],
                confidence=normalized_response['confidence']
            )
        except ValidationError as ve:
            logging.error(f"Validation error creating Decision: {str(ve)}")
            logging.error(f"Normalized response: {normalized_response}")
            return None, None, prompt, {}

        extra_data = {}
        if 'risk_factors' in normalized_response and normalized_response['risk_factors']:
            extra_data['risk_factors'] = normalized_response['risk_factors']

        meta_data = MetaData(
            model=getattr(response, 'model', None),
            created_at=getattr(response, 'created_at', None),
            done_reason=getattr(response, 'done_reason', None),
            done=getattr(response, 'done', None),
            total_duration=getattr(response, 'total_duration', None),
            load_duration=getattr(response, 'load_duration', None),
            prompt_eval_count=getattr(response, 'prompt_eval_count', None),
            prompt_eval_duration=getattr(response, 'prompt_eval_duration', None),
            eval_count=getattr(response, 'eval_count', None),
            eval_duration=getattr(response, 'eval_duration', None)
        )

        return decision, meta_data, prompt, extra_data

    except Exception as e:
        logging.error(f"Error during API call: {str(e)}")
        return None, None, prompt, {}

async def get_decision_with_timeout(
    prompt_type: PromptType,
    model_name: str,
    config: Config,
    prompt: str
) -> Tuple[Optional[Decision], Optional[MetaData], str, Dict[str, Any], TimeoutMetrics]:
    """Wraps get_decision with timeout handling."""
    timeout_metrics = TimeoutMetrics(
        occurred=False,
        retry_count=0,
        total_timeout_duration=0.0
    )
    # timeout_seconds = float(config.model_parameters.get("api_timeout", 30.0))
    timeout_seconds = float(config.model_ensemble[model_name]["max_response_time"])

    try:
        logging.debug(f"Calling get_decision with prompt: {prompt[:50]}...")
        decision, meta_data, used_prompt, extra_data = await asyncio.wait_for(
            get_decision(prompt_type, model_name, config, prompt),
            timeout=timeout_seconds
        )
        logging.debug("Received response from get_decision.")
        return decision, meta_data, used_prompt, extra_data, timeout_metrics

    except asyncio.TimeoutError:
        logging.warning("API call timed out while waiting for model response...")
        timeout_metrics.occurred = True
        timeout_metrics.retry_count += 1
        return None, None, prompt, {}, timeout_metrics

    except Exception as e:
        logging.error(f"Error during API call: {str(e)}")
        return None, None, prompt, {}, timeout_metrics

def save_decision(
    decision: Decision,
    meta_data: MetaData,
    prompt_type: PromptType,
    model_name: str,
    row_id: int,
    actual_value: str,
    config: Config,
    used_prompt: str,
    repeat_index: int = 0,
    extra_data: Dict[str, Any] = None
) -> bool:
    """Save decision and metadata to filesystem."""
    if extra_data is None:
        extra_data = {}

    try:
        clean_name = clean_model_name(model_name)
        output_dir = Path(config.output["base_dir"]) / clean_name
        output_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        output_path = generate_output_path(
            model_name=model_name,
            prompt_type=prompt_type,
            row_id=row_id,
            repeat_index=repeat_index,
            timestamp=timestamp,
            config=config,
            output_dir=output_dir,
            nshot_ct=config.execution.nshot_ct if prompt_type == PromptType.COT_NSHOT else None
        )

        decision_data = pydantic_or_dict(decision)
        meta_data_data = pydantic_or_dict(meta_data)

        decision_data.update({
            'id': row_id,
            'actual': actual_value,
            'correct': "YES" if decision.prediction.upper() == actual_value.upper() else "NO"
        })

        meta_data_data = convert_ns_to_s(meta_data_data)

        combined_data = {
            "decision": decision_data,
            "meta_data": meta_data_data,
            "evaluation": {
                "timestamp": timestamp,
                "model": model_name,
                "prompt_type": str(prompt_type),
                "row_id": row_id,
                "prediction_matches_actual": decision_data['correct'],
                "repeat_index": repeat_index
            }
        }

        if prompt_type == PromptType.COT_NSHOT:
            combined_data["evaluation"]["nshot_ct"] = config.execution.nshot_ct

        if 'risk_factors' in extra_data:
            combined_data["risk_factors"] = extra_data["risk_factors"]
        
        combined_data["prompt"] = used_prompt

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(combined_data, f, indent=2, default=str)

        logging.info(f"Successfully saved decision+meta_data to {output_path}")
        return True

    except Exception as e:
        logging.error(f"Error saving decision: {str(e)}")
        return False

###CODE: main.py
# main.py

import asyncio
import logging
import time
from typing import List, Optional, Set, Tuple, Dict
from pathlib import Path

from config import load_config, Config
from data_manager import DataManager
from prompt_manager import PromptManager
from models import PromptType
from metrics import PromptMetrics, TimeoutMetrics
from performance import PerformanceTracker, PerformanceStats, save_aggregate_stats
from utils import (
    clean_model_name,
    get_completion_status,
    get_next_sample,
    get_completion_counts,
    is_combination_fully_complete
)
from decision import (
    get_decision_with_timeout,
    save_decision
)

DELAY_BETWEEN_PROMPT_TYPES_SEC = 2
DELAY_BETWEEN_MODEL_LOAD_SEC = 10

async def cleanup_model():
    """Stop any running model instances without removing the model."""
    try:
        # Give a pause to allow resources to be freed
        await asyncio.sleep(2)
        
        # Force Python garbage collection
        import gc
        gc.collect()
        
        logging.info("Cleaned up model resources")
    except Exception as e:
        logging.warning(f"Error during model cleanup: {str(e)}")

def build_final_prompt(config: Config, base_prompt: str) -> str:
    """Conditionally prepend and/or append prefix and suffix to the base prompt."""
    return (
        (config.flags.prompt_prefix if config.flags.FLAG_PROMPT_PREFIX else "") +
        base_prompt +
        (config.flags.prompt_suffix if config.flags.FLAG_PROMPT_SUFFIX else "")
    )

async def run_evaluation_cycle(
    model_name: str,
    prompt_type: PromptType,
    config: Config,
    tracker: PerformanceTracker,
    data_manager: DataManager,
    prompt_manager: PromptManager
) -> None:
    """
    Run an evaluation cycle with configurable iterations and file checking.
    Now supports proper completion checking and restart capability.
    """
    max_samples = config.flags.max_samples
    max_calls = config.execution.max_calls_per_prompt
    batch_size = config.batch_size

    # Check overall completion status
    is_complete, existing_calls = get_completion_status(
        output_dir=Path(config.output["base_dir"]),
        model_name=model_name,
        prompt_type=str(prompt_type),
        max_samples=max_samples,
        max_calls=max_calls
    )

    if is_complete:
        logging.info(
            f"Skipping completed combination: {model_name} with {prompt_type} "
            f"(found {len(existing_calls)} samples with up to {max_calls} calls each)"
        )
        return

    # Calculate how many more samples we need
    completed_samples = sum(1 for calls in existing_calls.values() if calls >= max_calls)
    remaining_samples = max_samples - completed_samples

    if remaining_samples <= 0:
        logging.info(f"No additional samples needed for {model_name} with {prompt_type}")
        return

    dataset_info = data_manager.get_dataset_info()
    total_samples = min(
        dataset_info['dataset_sizes']['train'],
        remaining_samples
    )

    logging.info(
        f"Starting evaluation cycle for {model_name} with {prompt_type}, "
        f"processing {total_samples} more samples, batch size {batch_size}, "
        f"up to {max_calls} calls each"
    )

    processed_samples = 0
    
    while processed_samples < total_samples:
        remaining = total_samples - processed_samples
        current_batch_size = min(batch_size, remaining)
        
        try:
            data_batch = data_manager.get_batch(current_batch_size, dataset='train')
            
            for sample in data_batch:
                if processed_samples >= total_samples:
                    break
                    
                row_id = sample['id']
                actual_value = sample['target']
                
                # Get next needed repeat index for this sample
                next_repeat = get_next_sample(existing_calls, max_calls, row_id)
                if next_repeat is None:
                    continue  # Sample is complete
                
                # Process only remaining needed calls
                for repeat_index in range(next_repeat, max_calls):
                    start_time = time.time()
                    logging.info(
                        f"Processing sample {processed_samples + 1}/{total_samples} "
                        f"(ID: {row_id}), repeat #{repeat_index + 1}/{max_calls}"
                    )

                    try:
                        prompt = prompt_manager.get_prompt(prompt_type, row_id)
                        final_prompt = build_final_prompt(config, prompt)

                        decision, meta_data, used_prompt, extra_data, timeout_metrics = (
                            await get_decision_with_timeout(
                                prompt_type=prompt_type,
                                model_name=model_name,
                                config=config,
                                prompt=final_prompt
                            )
                        )

                        execution_time = time.time() - start_time

                        if decision is not None:
                            save_success = save_decision(
                                decision=decision,
                                meta_data=meta_data,
                                prompt_type=prompt_type,
                                model_name=model_name,
                                row_id=row_id,
                                actual_value=actual_value,
                                config=config,
                                used_prompt=used_prompt,
                                repeat_index=repeat_index,
                                extra_data=extra_data
                            )
                            
                            if not save_success:
                                logging.warning("Decision valid but save failed")
                            else:
                                existing_calls[row_id] = existing_calls.get(row_id, 0) + 1

                            metrics = PromptMetrics(
                                attempt_number=(processed_samples + 1) * 1000 + repeat_index + 1,
                                execution_time_seconds=execution_time,
                                successful=True,
                                timeout_metrics=timeout_metrics,
                                prediction=decision.prediction,
                                confidence=decision.confidence,
                                meta_data=meta_data.model_dump() if meta_data else {}
                            )
                        else:
                            metrics = PromptMetrics(
                                attempt_number=(processed_samples + 1) * 1000 + repeat_index + 1,
                                execution_time_seconds=execution_time,
                                successful=False,
                                timeout_metrics=timeout_metrics,
                                error_message="No valid decision received"
                            )

                        tracker.record_attempt(metrics)

                    except Exception as e:
                        logging.error(f"Error processing sample {row_id} (repeat {repeat_index}): {str(e)}")
                        continue  # Continue to next repeat attempt
                
                processed_samples += 1
                if processed_samples >= total_samples:
                    break

        except Exception as e:
            logging.error(f"Error processing batch: {str(e)}")
            break

    completed_after = sum(1 for calls in existing_calls.values() if calls >= max_calls)
    logging.info(
        f"Evaluation cycle completed. Processed {processed_samples} samples. "
        f"Total completed samples: {completed_after}"
    )

async def run_evaluation_session(
    model_name: str,
    prompt_type: PromptType,
    config: Config,
    data_manager: DataManager,
    prompt_manager: PromptManager
) -> Optional[PerformanceStats]:
    """Run a single model/prompt evaluation session."""
    tracker = PerformanceTracker(prompt_type, model_name)
    session_start = time.time()

    try:
        await run_evaluation_cycle(
            model_name,
            prompt_type,
            config,
            tracker,
            data_manager,
            prompt_manager
        )
        session_duration = time.time() - session_start

        if len(tracker.attempts) > 0:  # Only save metrics if we have attempts
            tracker.save_metrics(session_duration)
            stats = tracker._generate_stats()
            if stats is not None:
                return stats
            
        logging.info(f"No stats generated for {model_name} - {prompt_type}")
        return None

    except Exception as e:
        logging.error(f"Critical error in evaluation session: {e}", exc_info=True)
        return None
    
def is_model_combination_complete(
    output_dir: Path,
    model_name: str,
    prompt_type: str,
    max_samples: int,
    max_calls: int
) -> bool:
    """Check if a model/prompt combination has completed all required samples."""
    clean_name = clean_model_name(model_name)
    model_dir = output_dir / clean_name
    
    if not model_dir.exists():
        return False
        
    # Track completed samples for this combination
    completed_samples = set()
    required_calls = {}
    
    # Check all output files for this combination
    for file_path in model_dir.glob(f"{model_name}_{prompt_type}_id*_*.json"):
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
                row_id = data['decision']['id']
                completed_samples.add(row_id)
                required_calls[row_id] = required_calls.get(row_id, 0) + 1
        except (json.JSONDecodeError, KeyError) as e:
            logging.warning(f"Error reading file {file_path}: {e}")
            continue
    
    # Check if we have enough samples with enough calls each
    fully_completed = sum(1 for calls in required_calls.values() if calls >= max_calls)
    return fully_completed >= max_samples

async def main():
    """Main orchestrator with improved model management and restartability."""
    config = load_config("config.yaml")
    
    # Setup logging
    logging.basicConfig(
        level=getattr(logging, config.logging["level"].upper()),
        format=config.logging["format"],
        handlers=[
            logging.FileHandler(config.logging["file"]),
            logging.StreamHandler()
        ]
    )

    logging.info("Starting evaluation process")
    overall_start = time.time()

    try:
        data_manager = DataManager(config)
        data_manager.load_and_prepare_data()

        output_base = Path(config.output["base_dir"])
        output_base.mkdir(parents=True, exist_ok=True)

        prompt_manager = PromptManager(config, data_manager)
        session_results: List[PerformanceStats] = []
        
        # Track processed combinations to avoid duplicates
        processed_combinations: Set[Tuple[str, str]] = set()
        
        # First, scan all existing output files
        combination_completion_status: Dict[Tuple[str, str], Dict] = {}
        
        # Pre-scan completion status for all combinations
        for model_name, model_cfg in config.model_ensemble.items():
            for p_type in PromptType:
                combo_key = (model_name, str(p_type))
                
                # Skip if we've already processed this combination
                if combo_key in processed_combinations:
                    logging.info(f"Skipping duplicate model+prompt combination: {model_name} with {p_type}")
                    continue
                
                # Get completion counts for this combination
                completion_counts = get_completion_counts(
                    output_base,
                    model_name,
                    str(p_type)
                )
                
                is_complete = is_combination_fully_complete(
                    completion_counts,
                    config.flags.max_samples,
                    config.execution.max_calls_per_prompt
                )
                
                combination_completion_status[combo_key] = {
                    'is_complete': is_complete,
                    'completion_counts': completion_counts
                }

        # Now process only incomplete combinations
        for model_name, model_cfg in config.model_ensemble.items():
            logging.info(f"Starting evaluation of model: {model_name}")
            model_name_clean = clean_model_name(model_name)
            
            # Create model-specific output directory
            model_output_dir = output_base / model_name_clean
            model_output_dir.mkdir(parents=True, exist_ok=True)
            
            # Iterate over prompt types
            for p_type in PromptType:
                combo_key = (model_name, str(p_type))
                
                # Skip if already processed or complete
                if combo_key in processed_combinations:
                    logging.info(f"Skipping already processed combination: {model_name} with {p_type}")
                    continue
                
                status = combination_completion_status.get(combo_key, {
                    'is_complete': False,
                    'completion_counts': {}
                })
                
                if status['is_complete']:
                    logging.info(f"Skipping completed combination: {model_name} with {p_type}")
                    continue
                
                logging.info(f"Processing combination: {model_name} with {p_type}")
                
                # Run evaluation
                stats = await run_evaluation_session(
                    model_name=model_name,
                    prompt_type=p_type,
                    config=config,
                    data_manager=data_manager,
                    prompt_manager=prompt_manager
                )
                
                if stats:
                    session_results.append(stats)
                
                # Mark as processed
                processed_combinations.add(combo_key)
                
                # Cleanup and pause
                await cleanup_model()
                await asyncio.sleep(DELAY_BETWEEN_PROMPT_TYPES_SEC)
            
            # Longer pause between models
            await asyncio.sleep(DELAY_BETWEEN_MODEL_LOAD_SEC)

        # Save final results
        total_duration = time.time() - overall_start
        if session_results:
            save_aggregate_stats(session_results, total_duration)
            logging.info("Successfully saved aggregate statistics")
        else:
            logging.warning("No session results to aggregate")

        logging.info(f"All evaluations completed in {total_duration:.2f} seconds")

    except Exception as e:
        logging.error(f"Fatal error in main: {e}", exc_info=True)
        raise

if __name__ == "__main__":
    asyncio.run(main())

###INSTRUCTIONS:


